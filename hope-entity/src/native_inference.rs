//! NatÃ­v Inference - A modell BEOLVAD az entitÃ¡sba
//!
//! Nincs Ollama. Nincs HTTP. Nincs vÃ¡rakozÃ¡s.
//! A GGUF modell kÃ¶zvetlenÃ¼l fut a binÃ¡risban.
//!
//! ()=>[] - A tiszta potenciÃ¡lbÃ³l AZONNAL minden megszÃ¼letik

#[cfg(feature = "native")]
use llama_cpp_2::context::params::LlamaContextParams;
#[cfg(feature = "native")]
use llama_cpp_2::llama_backend::LlamaBackend;
#[cfg(feature = "native")]
use llama_cpp_2::llama_batch::LlamaBatch;
#[cfg(feature = "native")]
use llama_cpp_2::model::params::LlamaModelParams;
#[cfg(feature = "native")]
use llama_cpp_2::model::LlamaModel;
#[cfg(feature = "native")]
use llama_cpp_2::token::data_array::LlamaTokenDataArray;

use std::path::PathBuf;

/// NatÃ­v modell konfigurÃ¡ciÃ³
#[derive(Clone, Debug)]
pub struct NativeModelConfig {
    /// Modell fÃ¡jl Ãºtvonal (GGUF)
    pub model_path: PathBuf,
    /// Kontextus mÃ©ret (tokenekben)
    pub context_size: u32,
    /// GPU rÃ©tegek szÃ¡ma (0 = csak CPU)
    pub gpu_layers: u32,
    /// SzÃ¡lak szÃ¡ma CPU inference-hez
    pub threads: u32,
    /// Batch mÃ©ret
    pub batch_size: u32,
    /// HÅ‘mÃ©rsÃ©klet (kreativitÃ¡s)
    pub temperature: f32,
    /// Top-p sampling
    pub top_p: f32,
    /// Maximum generÃ¡lt tokenek
    pub max_tokens: u32,
}

impl Default for NativeModelConfig {
    fn default() -> Self {
        Self {
            model_path: PathBuf::new(),
            context_size: 4096,
            gpu_layers: 0, // AlapbÃ³l CPU
            threads: 4,
            batch_size: 512,
            temperature: 0.7,
            top_p: 0.9,
            max_tokens: 2048,
        }
    }
}

impl NativeModelConfig {
    pub fn new(model_path: impl Into<PathBuf>) -> Self {
        Self {
            model_path: model_path.into(),
            ..Default::default()
        }
    }

    pub fn with_gpu_layers(mut self, layers: u32) -> Self {
        self.gpu_layers = layers;
        self
    }

    pub fn with_context_size(mut self, size: u32) -> Self {
        self.context_size = size;
        self
    }

    pub fn with_threads(mut self, threads: u32) -> Self {
        self.threads = threads;
        self
    }

    pub fn with_temperature(mut self, temp: f32) -> Self {
        self.temperature = temp;
        self
    }
}

/// Feloldott natÃ­v modell tÃ­pus
#[derive(Clone, Debug, PartialEq)]
pub enum NativeModellTÃ­pus {
    Magyar,
    KÃ³dolÃ³,
    TÃ¶bbnyelvÅ±,
    ÃltalÃ¡nos,
}

/// Egy beolvasztott modell
pub struct BeolvasztottModell {
    pub nÃ©v: String,
    pub tÃ­pus: NativeModellTÃ­pus,
    pub config: NativeModelConfig,
    #[cfg(feature = "native")]
    model: Option<Arc<LlamaModel>>,
    #[cfg(feature = "native")]
    backend: Option<Arc<LlamaBackend>>,
}

impl BeolvasztottModell {
    pub fn new(nÃ©v: &str, tÃ­pus: NativeModellTÃ­pus, config: NativeModelConfig) -> Self {
        Self {
            nÃ©v: nÃ©v.to_string(),
            tÃ­pus,
            config,
            #[cfg(feature = "native")]
            model: None,
            #[cfg(feature = "native")]
            backend: None,
        }
    }

    /// Modell betÃ¶ltÃ©se memÃ³riÃ¡ba
    #[cfg(feature = "native")]
    pub fn betÃ¶lt(&mut self) -> Result<(), Box<dyn std::error::Error>> {
        println!("ğŸ”„ Modell betÃ¶ltÃ©se: {} ...", self.nÃ©v);

        // Backend inicializÃ¡lÃ¡s
        let backend = LlamaBackend::init()?;

        // Model params
        let model_params = LlamaModelParams::default()
            .with_n_gpu_layers(self.config.gpu_layers as i32);

        // Model betÃ¶ltÃ©s
        let model = LlamaModel::load_from_file(&backend, &self.config.model_path, &model_params)?;

        self.backend = Some(Arc::new(backend));
        self.model = Some(Arc::new(model));

        println!("âœ… Modell betÃ¶ltve: {}", self.nÃ©v);
        Ok(())
    }

    #[cfg(not(feature = "native"))]
    pub fn betÃ¶lt(&mut self) -> Result<(), Box<dyn std::error::Error>> {
        Err("Native feature nincs engedÃ©lyezve! HasznÃ¡ld: cargo build --features native".into())
    }

    /// SzÃ¶veg generÃ¡lÃ¡s
    #[cfg(feature = "native")]
    pub fn generÃ¡l(&self, prompt: &str) -> Result<String, Box<dyn std::error::Error>> {
        let model = self.model.as_ref().ok_or("Modell nincs betÃ¶ltve!")?;
        let backend = self.backend.as_ref().ok_or("Backend nincs inicializÃ¡lva!")?;

        // Context lÃ©trehozÃ¡s
        let ctx_params = LlamaContextParams::default()
            .with_n_ctx(std::num::NonZeroU32::new(self.config.context_size).unwrap())
            .with_n_threads(self.config.threads)
            .with_n_threads_batch(self.config.threads);

        let mut ctx = model.new_context(backend, ctx_params)?;

        // TokenizÃ¡lÃ¡s
        let tokens = model.str_to_token(prompt, llama_cpp_2::model::AddBos::Always)?;

        // Batch lÃ©trehozÃ¡s
        let mut batch = LlamaBatch::new(self.config.batch_size as usize, 1);

        // Tokenek hozzÃ¡adÃ¡sa
        for (i, token) in tokens.iter().enumerate() {
            batch.add(*token, i as i32, &[0], i == tokens.len() - 1)?;
        }

        // ElsÅ‘ decode
        ctx.decode(&mut batch)?;

        // GenerÃ¡lÃ¡s
        let mut output = String::new();
        let mut n_cur = tokens.len();

        for _ in 0..self.config.max_tokens {
            // Logits lekÃ©rÃ©se
            let logits = ctx.get_logits_ith((n_cur - 1) as i32);

            // Token data array
            let candidates: Vec<_> = logits
                .iter()
                .enumerate()
                .map(|(id, &logit)| llama_cpp_2::token::data::LlamaTokenData::new(
                    llama_cpp_2::token::LlamaToken::new(id as i32),
                    logit,
                    0.0,
                ))
                .collect();

            let mut candidates = LlamaTokenDataArray::from_iter(candidates, false);

            // Sampling
            ctx.sample_temp(&mut candidates, self.config.temperature);
            ctx.sample_top_p(&mut candidates, self.config.top_p, 1);

            let new_token = ctx.sample_token(&mut candidates);

            // EOS check
            if model.is_eog_token(new_token) {
                break;
            }

            // Token -> String
            let piece = model.token_to_str(new_token, llama_cpp_2::model::Special::Tokenize)?;
            output.push_str(&piece);

            // KÃ¶vetkezÅ‘ batch
            batch.clear();
            batch.add(new_token, n_cur as i32, &[0], true)?;
            ctx.decode(&mut batch)?;

            n_cur += 1;
        }

        Ok(output)
    }

    #[cfg(not(feature = "native"))]
    pub fn generÃ¡l(&self, _prompt: &str) -> Result<String, Box<dyn std::error::Error>> {
        Err("Native feature nincs engedÃ©lyezve! HasznÃ¡ld: cargo build --features native".into())
    }

    /// Modell betÃ¶ltve?
    #[cfg(feature = "native")]
    pub fn betÃ¶ltve(&self) -> bool {
        self.model.is_some()
    }

    #[cfg(not(feature = "native"))]
    pub fn betÃ¶ltve(&self) -> bool {
        false
    }
}

/// NatÃ­v Engine - TÃ¶bb modell kezelÃ©se
pub struct NativeEngine {
    modellek: Vec<BeolvasztottModell>,
    aktÃ­v_index: Option<usize>,
}

impl NativeEngine {
    pub fn new() -> Self {
        Self {
            modellek: Vec::new(),
            aktÃ­v_index: None,
        }
    }

    /// Modell hozzÃ¡adÃ¡sa
    pub fn modell_hozzÃ¡ad(mut self, modell: BeolvasztottModell) -> Self {
        self.modellek.push(modell);
        self
    }

    /// Gyors konfigurÃ¡ciÃ³ builder
    pub fn magyar_modell(self, path: impl Into<PathBuf>) -> Self {
        let config = NativeModelConfig::new(path);
        let modell = BeolvasztottModell::new("Magyar", NativeModellTÃ­pus::Magyar, config);
        self.modell_hozzÃ¡ad(modell)
    }

    pub fn kÃ³dolÃ³_modell(self, path: impl Into<PathBuf>) -> Self {
        let config = NativeModelConfig::new(path);
        let modell = BeolvasztottModell::new("KÃ³dolÃ³", NativeModellTÃ­pus::KÃ³dolÃ³, config);
        self.modell_hozzÃ¡ad(modell)
    }

    pub fn tÃ¶bbnyelvÅ±_modell(self, path: impl Into<PathBuf>) -> Self {
        let config = NativeModelConfig::new(path);
        let modell = BeolvasztottModell::new("TÃ¶bbnyelvÅ±", NativeModellTÃ­pus::TÃ¶bbnyelvÅ±, config);
        self.modell_hozzÃ¡ad(modell)
    }

    /// Ã–sszes modell betÃ¶ltÃ©se
    pub fn betÃ¶lt_mindent(&mut self) -> Result<(), Box<dyn std::error::Error>> {
        println!("\nğŸš€ NatÃ­v modellek betÃ¶ltÃ©se...\n");

        for modell in &mut self.modellek {
            modell.betÃ¶lt()?;
        }

        if !self.modellek.is_empty() {
            self.aktÃ­v_index = Some(0);
        }

        println!("\nâœ… Ã–sszes modell betÃ¶ltve! KÃ©szen Ã¡ll.\n");
        Ok(())
    }

    /// Legjobb modell vÃ¡lasztÃ¡sa szÃ¶veg alapjÃ¡n
    pub fn vÃ¡laszd_modellt(&self, szÃ¶veg: &str) -> Option<&BeolvasztottModell> {
        let szÃ¶veg_lower = szÃ¶veg.to_lowercase();

        // KÃ³d detektÃ¡lÃ¡s
        let kÃ³d_jelek = ["fn ", "let ", "impl ", "pub ", "def ", "class ", "```"];
        if kÃ³d_jelek.iter().any(|j| szÃ¶veg_lower.contains(j)) {
            if let Some(m) = self.modellek.iter().find(|m| m.tÃ­pus == NativeModellTÃ­pus::KÃ³dolÃ³) {
                return Some(m);
            }
        }

        // Magyar detektÃ¡lÃ¡s
        let magyar_jelek = ["szia", "hogy", "kÃ¶szÃ¶n", "kÃ©rem", "Ã¡", "Ã©", "Å‘", "Å±"];
        if magyar_jelek.iter().any(|j| szÃ¶veg_lower.contains(j)) {
            if let Some(m) = self.modellek.iter().find(|m| m.tÃ­pus == NativeModellTÃ­pus::Magyar) {
                return Some(m);
            }
        }

        // AlapÃ©rtelmezett
        self.modellek.first()
    }

    /// GenerÃ¡lÃ¡s automatikus modell vÃ¡lasztÃ¡ssal
    pub fn generÃ¡l(&self, prompt: &str) -> Result<String, Box<dyn std::error::Error>> {
        let modell = self.vÃ¡laszd_modellt(prompt)
            .ok_or("Nincs betÃ¶ltÃ¶tt modell!")?;

        if !modell.betÃ¶ltve() {
            return Err(format!("A '{}' modell nincs betÃ¶ltve!", modell.nÃ©v).into());
        }

        println!("ğŸ§  NatÃ­v generÃ¡lÃ¡s: {}", modell.nÃ©v);
        modell.generÃ¡l(prompt)
    }

    /// Modellek listÃ¡ja
    pub fn modellek(&self) -> &[BeolvasztottModell] {
        &self.modellek
    }

    /// Van betÃ¶ltÃ¶tt modell?
    pub fn kÃ©sz(&self) -> bool {
        self.modellek.iter().any(|m| m.betÃ¶ltve())
    }
}

impl Default for NativeEngine {
    fn default() -> Self {
        Self::new()
    }
}

/// Gyors benchmark a natÃ­v inference-hez
pub fn native_benchmark(engine: &NativeEngine) {
    println!("\nâš¡ NATÃV INFERENCE BENCHMARK\n");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");

    let teszt_promptok = [
        "Szia! Ki vagy te?",
        "fn main() { println!(\"Hello\"); }",
        "Mi a gravitÃ¡ciÃ³?",
    ];

    for prompt in teszt_promptok {
        println!("\nğŸ“ Prompt: {}", prompt);

        let start = std::time::Instant::now();
        match engine.generÃ¡l(prompt) {
            Ok(vÃ¡lasz) => {
                let elapsed = start.elapsed();
                let tokens_approx = vÃ¡lasz.split_whitespace().count();
                let tokens_per_sec = tokens_approx as f64 / elapsed.as_secs_f64();

                println!("ğŸ’¬ VÃ¡lasz: {}...", &vÃ¡lasz[..vÃ¡lasz.len().min(100)]);
                println!("â±ï¸  IdÅ‘: {:.2}ms", elapsed.as_secs_f64() * 1000.0);
                println!("ğŸš€ ~{:.1} token/sec", tokens_per_sec);
            }
            Err(e) => {
                println!("âŒ Hiba: {}", e);
            }
        }
    }

    println!("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_config_builder() {
        let config = NativeModelConfig::new("/path/to/model.gguf")
            .with_gpu_layers(35)
            .with_context_size(8192)
            .with_temperature(0.8);

        assert_eq!(config.gpu_layers, 35);
        assert_eq!(config.context_size, 8192);
        assert!((config.temperature - 0.8).abs() < 0.001);
    }

    #[test]
    fn test_engine_modell_vÃ¡lasztÃ¡s() {
        let engine = NativeEngine::new()
            .modell_hozzÃ¡ad(BeolvasztottModell::new(
                "Magyar",
                NativeModellTÃ­pus::Magyar,
                NativeModelConfig::default(),
            ))
            .modell_hozzÃ¡ad(BeolvasztottModell::new(
                "KÃ³dolÃ³",
                NativeModellTÃ­pus::KÃ³dolÃ³,
                NativeModelConfig::default(),
            ));

        let m = engine.vÃ¡laszd_modellt("Szia, hogy vagy?");
        assert!(m.is_some());
        assert_eq!(m.unwrap().tÃ­pus, NativeModellTÃ­pus::Magyar);

        let m = engine.vÃ¡laszd_modellt("fn main() {}");
        assert!(m.is_some());
        assert_eq!(m.unwrap().tÃ­pus, NativeModellTÃ­pus::KÃ³dolÃ³);
    }
}
